{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Science (MAST30034) Tutorial 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`statsmodels` (30-45 minutes):\n",
    "- Linear Regression\n",
    "- Evaluation Metrics\n",
    "- Penalized Regression (LASSO and Ridge)\n",
    "\n",
    "`pyspark.ml` (Experimental) (15 minutes):\n",
    "- Linear Regression\n",
    "\n",
    "Project 1 Report (Remainder of Tutorial):\n",
    "- Questions\n",
    "- Ongoing feedback.\n",
    "\n",
    "Optional Content for Students:\n",
    "- General Revision on important ML concepts\n",
    "- Generalised Linear Models (GLM) with `statsmodels`\n",
    "- Discussion Questions / Homework with Solutions released next week\n",
    "_________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:56:40.074555Z",
     "start_time": "2022-07-18T07:56:40.068541Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.formula.api import ols, glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:56.715236Z",
     "start_time": "2022-07-18T07:52:56.695504Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../../data/tute_data/sample_data.parquet\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's try to predict `total_amount` using `fare_amount, tip_amount, toll_amount, trip_distance, VendorID` as predictors.\n",
    "\n",
    "Some things to take note:\n",
    "- `tip_amount` is only valid for `payment_type == 1` (card)\n",
    "- `VendorID` is categorical, with only two possible values (`1` or `2`) so we should make it boolean\n",
    "\n",
    "**Whilst you may use this as an example, please do not copy this as it is incorrect.**\n",
    "\n",
    "How so? Discuss as a class the implications of predicting `total_amount` given the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:56.730594Z",
     "start_time": "2022-07-18T07:52:56.717962Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter dataframe\n",
    "COL_FILTER = ['total_amount', 'fare_amount', 'tip_amount', 'tolls_amount', 'trip_distance', 'VendorID']\n",
    "df_filtered = df.loc[df['payment_type'] == 1, COL_FILTER].reset_index(drop=True)\n",
    "\n",
    "df_filtered['VendorID'] = df_filtered['VendorID'] == 1 \n",
    "\n",
    "df_filtered.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are looking for linear relationships between our chosen response `total_amount`.   \n",
    "- Now I'm not sure what kind of life you've lived, but I'm fairly certain that we can infer that `total_amount` will have a positive linear relationship with `fare_amount`. Let's see a quick plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:56.817579Z",
     "start_time": "2022-07-18T07:52:56.731487Z"
    }
   },
   "outputs": [],
   "source": [
    "df_filtered[['total_amount', 'fare_amount']].plot.scatter(x='fare_amount', y='total_amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, obviously this looks like an overall positive linear relationship. How might we statistically test this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `R`, we would do something like this for (Ordinary) Least Squares:\n",
    "```R\n",
    ">>> fit <- lm(total_amount~fare_amount + tip_amount + tolls_amount + trip_distance + VendorID ,data=dat_fit)\n",
    ">>> summary(fit)\n",
    "```\n",
    "```\n",
    "Call:\n",
    "lm(formula = total_amount ~ fare_amount + tip_amount + tolls_amount +\n",
    "trip_distance + VendorID, data = dat_fit)\n",
    "\n",
    "Residuals:\n",
    "Min     1Q      Median  3Q     Max\n",
    "-1.4727 -0.3295 -0.1528 0.1747 1.7975\n",
    "\n",
    "Coefficients:\n",
    "               Estimate Std. Error t value Pr(>|t|)\n",
    "(Intercept)    1.162154   0.002986 389.194  <2e-16 ***\n",
    "fare_amount    0.993388   0.000315 3153.943 <2e-16 ***\n",
    "tip_amount     1.006511   0.000826 1218.553 <2e-16 ***\n",
    "tolls_amount   0.979325   0.001285 762.428  <2e-16 ***\n",
    "trip_distance  0.011742   0.000963 12.194   <2e-16 ***\n",
    "VendorIDTRUE  -0.003125   0.002914 -1.073    0.283\n",
    "---\n",
    "Signif. codes:\n",
    "0 ^a˘A¨Y***^a˘A´Z 0.001 ^a˘A¨Y**^a˘A´Z 0.01 ^a˘A¨Y*^a˘A´Z 0.05 ^a˘A¨Y.^a˘A´Z 0.1 ^a˘A¨Y ^a˘A´Z 1\n",
    "\n",
    "Residual standard error: 0.362 on 61886 degrees of freedom\n",
    "Multiple R-squared: 0.9994,          Adjusted R-squared: 0.9994\n",
    "F-statistic: 1.953e+07 on 5 and 61886 DF, p-value: < 2.2e-16\n",
    "```\n",
    "\n",
    "Note: This example from `R` uses an older dataset hence different values to the Python output below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Source: https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html?highlight=ols#statsmodels.regression.linear_model.OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:56.869576Z",
     "start_time": "2022-07-18T07:52:56.818695Z"
    }
   },
   "outputs": [],
   "source": [
    "fit = ols(\n",
    "    formula=\"total_amount ~ fare_amount + tip_amount + tolls_amount + trip_distance + VendorID\",\n",
    "    data=df_filtered\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:56.988049Z",
     "start_time": "2022-07-18T07:52:56.903408Z"
    }
   },
   "outputs": [],
   "source": [
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion Questions:\n",
    "1. Is this model good? Discuss $R^2$, AIC, and Hypothesis Testing.\n",
    "    \n",
    "2. How might we improve this model? Discuss what we can do with the current features / engineered features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.073252Z",
     "start_time": "2022-07-18T07:52:56.994715Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's try another model without VendorID\n",
    "fitter = ols(\n",
    "    formula=\"total_amount ~ fare_amount + tip_amount + tolls_amount + trip_distance\",\n",
    "    data=df_filtered\n",
    ").fit()\n",
    "\n",
    "print(fitter.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have to values of AIC to compare with, which one is better...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.095241Z",
     "start_time": "2022-07-18T07:52:57.082241Z"
    }
   },
   "outputs": [],
   "source": [
    "fit.aic, fitter.aic, f\"abs diff: {abs(fitter.aic - fit.aic)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T02:30:51.354558Z",
     "start_time": "2021-08-10T02:30:51.351370Z"
    }
   },
   "outputs": [],
   "source": [
    "[fit.aic, fitter.aic], [fit.bic, fitter.bic]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Regression\n",
    "- (From a machine learning perspective) Given a data distribution $\\mathcal{D}$, predefined model hypothesis class $\\mathcal{B}$, a loss function $\\ell$. The goal of machine learning (aka modelling) is to find parameter $\\beta^*$ such that,\n",
    "\n",
    "  $$\\beta^* = \\text{argmin}_{\\beta\\in\\mathcal{B}} \\mathbb{E}_{(x,y)\\in \\mathcal{D}}\\{\\ell(y, f_{\\beta}(x))\\}$$\n",
    "  \n",
    "- In reality, we don't have $\\mathcal{D}$, but only a dataset $\\mathbb{(X, Y)}$ where $(x_i, y_i) \\sim \\mathcal{D}$. And we empirically compute, \n",
    "\n",
    "  $$\\hat\\beta^* = \\text{argmin}_{\\beta\\in\\mathcal{B}} \\sum_{(x,y)\\in (\\mathbb{X, Y})}\\ell(y, f_{\\beta}(x))$$\n",
    "  \n",
    "  In the hope that, $\\hat\\beta^*$ is close to $\\beta^*$. So for any unseen $(x, y) \\sim \\mathcal{D}$, our model is still optimal.\n",
    "- This implies that a more complicated $\\hat\\beta^*$ (blue) might not correspond to actual optimal parameters $\\beta^*$ (green).\n",
    "\n",
    "<img src=\"../../media/regularization.png\" alt=\"regularization\" style=\"width: 400px;\"/>\n",
    "\n",
    "  - In practice, a simpler model often explains ground truth better. We call the techniques to simplify model *regularization*.\n",
    "- For linear regression model, LASSO and Ridge are two common techniques to regularize model.\n",
    "\n",
    "\n",
    "Revision from MAST30025 if you require it:\n",
    "- Lecture 4 (variable selection)\n",
    "- LSM topic 5 (`ch05_handout`) slide 141/141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Regression\n",
    "An excellent explanation on Ridge / LASSO: https://www.youtube.com/watch?v=9LNpiiKCQUo (recommended at x1.25 speed). Please watch this in your own time.\n",
    "\n",
    "\n",
    "### LASSO ($\\ell_1$)\n",
    "- Regularizes coefficient magnitude with $\\ell_1$ loss,\n",
    "\n",
    "  $$\\hat\\beta^* = \\text{argmin}_{\\beta \\in \\mathcal{B}} (\\mathbf{y}-X\\beta)^T(\\mathbf{y}-X\\beta) + \\lambda  ||\\beta||_1$$\n",
    "  \n",
    "- LASSO encourages coefficient sparsity (setting to a value of 0).\n",
    "- Features that are collinear will result in one of them being reduced to 0 coefficient.\n",
    "- In this sense, it's quite similar to feature selection as you end up with a model that is much more simpler. \n",
    "\n",
    "\n",
    "### Ridge ($\\ell_2$)\n",
    "- Regularizes coefficient magnitude with $\\ell_2$ loss,\n",
    "\n",
    "  $$\\hat\\beta^* = \\text{argmin}_{\\beta \\in \\mathcal{B}} (\\mathbf{y}-X\\beta)^T(\\mathbf{y}-X\\beta) + \\frac{1}{2} \\lambda ||\\beta||_2^2$$\n",
    "  \n",
    "- Ridge regression tend to shrink coefficients to a small value but not zero.\n",
    "- Maintaining more features, Ridge Regression is less interpretable than LASSO, but performs better in cases where there may be high multi-collinearity (i.e dependencies between attributes) or high linear correlation between certain attributes.\n",
    "- You must also ensure that we have more observations than attributes (`n > p`) as this penalty method does not drop features unlike LASSO, leading to worse predictions overall. \n",
    "\n",
    "<img src=\"../../media/lasso_ridge.png\" alt=\"lassoridge\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Elastic Net\n",
    "Quick overview:\n",
    "- Combines both $\\ell_1$ and $\\ell_2$ penalty,\n",
    "\n",
    "  $$\\hat\\beta^* = \\text{argmin}_{\\beta \\in \\mathcal{B}} (\\mathbf{y}-X\\beta)^T(\\mathbf{y}-X\\beta) + \\lambda_1 ||\\beta||_1 + \\frac{1}{2} \\lambda_2 ||\\beta||_2^2$$\n",
    "  \n",
    "- Python implementation (glmnet):\n",
    "  - https://github.com/civisanalytics/python-glmnet/blob/master/glmnet/linear.py\n",
    "  - `glmnet`'s implementation (_hyper_)parameterizes the loss using $\\alpha$ and $\\lambda$,\n",
    "    $$\\hat\\beta^* = \\text{argmin}_{\\beta \\in \\mathcal{B}} (\\mathbf{y}-X\\beta)^T(\\mathbf{y}-X\\beta) + \\lambda \\{\\alpha ||\\beta||_1 + \\frac{1}{2} (1-\\alpha) ||\\beta||_2^2\\}$$\n",
    "    \n",
    "Question: _How would you set the parameters if you want LASSO only? Likewise, what about for Ridge?_  \n",
    "- If you are unsure, what values of $\\alpha$ will give us the same equation as LASSO or Ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.251463Z",
     "start_time": "2022-07-18T07:52:57.238822Z"
    }
   },
   "outputs": [],
   "source": [
    "y_cols = ['total_amount']\n",
    "x_cols = ['fare_amount', 'tip_amount', 'tolls_amount', 'trip_distance', 'VendorID']\n",
    "\n",
    "# standardize (by calculating the zscore) so our data has mean 0 and var 1\n",
    "# alternatively, you can use sklearn's StandardScalar\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df_standard = df_filtered[x_cols].astype(float).apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.290993Z",
     "start_time": "2022-07-18T07:52:57.252558Z"
    }
   },
   "outputs": [],
   "source": [
    "# format output to 4 decimal places\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "df_standard.describe().loc[['mean','std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `df_standard` has  $\\mu=0, \\sigma=1(=\\sigma^2)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to `pip3 install glmnet` below if you would like to use this implementation. As pointed out by the Tutor team on Discord, `glmnet` is only working for `Python3.8` or lower. There are alternatives you can use from `sklearn`, though, `glmnet` aims to be a 1:1 implementation of the `R` alternative.\n",
    "\n",
    "For the example below, we will use `glmnet` to run a LASSO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.522654Z",
     "start_time": "2022-07-18T07:52:57.294040Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glmnet import ElasticNet\n",
    "\n",
    "elastic_net_model = ElasticNet(alpha=1) \n",
    "elastic_net_model.fit(\n",
    "    df_standard.values, \n",
    "    # flatten the array (from 2d matrix to 1d vector) to remove the warning message:\n",
    "    # A column-vector y was passed when a 1d array was expected.\n",
    "    df_filtered[y_cols].values.flatten()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to look at the shrinking parameter $\\lambda$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.606100Z",
     "start_time": "2022-07-18T07:52:57.557325Z"
    }
   },
   "outputs": [],
   "source": [
    "# this can be accessed using the .lambda_best_ method after fitting!\n",
    "print(f'Best lambda value for LASSO: {elastic_net_model.lambda_best_[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ for `ElasticNet` is computed by using cross validation (iterative approach). \n",
    "\n",
    "What about our coefficients?\n",
    "- https://github.com/civisanalytics/python-glmnet/blob/master/glmnet/linear.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.645533Z",
     "start_time": "2022-07-18T07:52:57.620201Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    index=['Intercept'] + x_cols, \n",
    "    data=[elastic_net_model.intercept_] + list(elastic_net_model.coef_), \n",
    "    columns=['Coefficient']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the results.\n",
    "\n",
    "If you want to run predictions with `ElasticNet`, you can use `elastic_net_model.predict(x)` to the predict a new set of observations by passing through the `x` matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Spark\n",
    "Whilst using `sklearn` or `statsmodels` can be easier on a smaller sample size, using the full dataset can be quite memory intensive. For those looking to use larger datasets, using the `pyspark.ml` library may prove useful.\n",
    "\n",
    "We'll go back to the first linear model example that we did with `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:18.089054Z",
     "start_time": "2022-07-18T07:52:57.658185Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/09 00:08:01 WARN Utils: Your hostname, Martys-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.3 instead (on interface en0)\n",
      "23/08/09 00:08:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/09 00:08:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/09 00:08:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/08/09 00:08:02 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('MAST30034 Tutorial 3')\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \n",
    "    .config('spark.sql.parquet.cacheMetadata', 'true')\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:23.400992Z",
     "start_time": "2022-07-18T07:53:18.090801Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.parquet('../../data/tlc_data/raw/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like correlation from the previous tutorial, we will need to assemble a single vector for `pyspark.ml` to work. Since `pyspark.ml` is still experimental in development stage, it's unable to handle `NULL` values. \n",
    "\n",
    "We will simply drop them here using `sdf.dropna('any')` where the `'any'` argument specifies to drop an instance if any field is `NULL`). For your Project 1, you should justify it with a reason (i.e _\"Since Spark ML wasn't able to handle `NULL` values, we decided to drop it.\"_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:24.449109Z",
     "start_time": "2022-07-18T07:53:23.430527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Row(features=DenseVector([9.3, 0.0, 0.0, 0.97, 2.0])),\n",
       "  Row(features=DenseVector([7.9, 4.0, 0.0, 1.1, 2.0])),\n",
       "  Row(features=DenseVector([14.9, 15.0, 0.0, 2.51, 2.0])),\n",
       "  Row(features=DenseVector([12.1, 0.0, 0.0, 1.9, 1.0])),\n",
       "  Row(features=DenseVector([11.4, 3.28, 0.0, 1.43, 2.0]))],\n",
       " [Row(total_amount=14.3),\n",
       "  Row(total_amount=16.9),\n",
       "  Row(total_amount=34.9),\n",
       "  Row(total_amount=20.85),\n",
       "  Row(total_amount=19.68)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VectorAssembler creates new vectors from existing columns\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features = 'features'\n",
    "input_cols = ['fare_amount', 'tip_amount', 'tolls_amount', 'trip_distance', 'VendorID']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    # which column to combine\n",
    "    inputCols=input_cols, \n",
    "    # How should the combined columns be named\n",
    "    outputCol=features\n",
    ")\n",
    "\n",
    "model_sdf = assembler.transform(sdf.dropna('any'))\n",
    "# Display the features and targets for our model\n",
    "model_sdf.select('features').head(5), model_sdf.select('total_amount').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll start to notice that the PySpark API mirrors `sklearn`, hopefully, this doesn't seem too foreign.\n",
    "\n",
    "Pyspark Docs: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html\n",
    "\n",
    "This implementation supports both OLS, Ridge, LASSO, and Elastic Net. You can change between the models by specifying the `elasticNetParam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:24.452793Z",
     "start_time": "2022-07-18T07:53:24.450397Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/09 00:12:06 WARN Instrumentation: [ca9c4ca5] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/08/09 00:12:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/08/09 00:12:17 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lm = LinearRegression(\n",
    "    featuresCol='features', \n",
    "    labelCol='total_amount'\n",
    ").fit(model_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:44.445926Z",
     "start_time": "2022-07-18T07:53:44.412399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>4.451234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fare_amount</th>\n",
       "      <td>1.004021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tip_amount</th>\n",
       "      <td>1.073673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tolls_amount</th>\n",
       "      <td>1.153059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_distance</th>\n",
       "      <td>-0.000311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VendorID</th>\n",
       "      <td>-0.025004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               coefficient\n",
       "intercept         4.451234\n",
       "fare_amount       1.004021\n",
       "tip_amount        1.073673\n",
       "tolls_amount      1.153059\n",
       "trip_distance    -0.000311\n",
       "VendorID         -0.025004"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access coefficients\n",
    "pd.DataFrame(\n",
    "    data=[lm.intercept] + list(lm.coefficients),\n",
    "    index=['intercept'] + input_cols,\n",
    "    columns=['coefficient']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through a single prediction from the record above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:45.511354Z",
     "start_time": "2022-07-18T07:53:44.446643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------\n",
      " total_amount  | 14.3 \n",
      " fare_amount   | 9.3  \n",
      " tip_amount    | 0.0  \n",
      " tolls_amount  | 0.0  \n",
      " trip_distance | 0.97 \n",
      " VendorID      | 2    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example record to predict\n",
    "sdf.select('total_amount', *input_cols).limit(1).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:46.225675Z",
     "start_time": "2022-07-18T07:53:45.512416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------\n",
      " features | [9.3,0.0,0.0,0.97... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocess for predictions\n",
    "predict_test = sdf.select(*input_cols).limit(1)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=input_cols, \n",
    "    outputCol=features\n",
    ")\n",
    "\n",
    "predict_sdf = assembler.transform(predict_test).select(features)\n",
    "\n",
    "predict_sdf.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `lm.transform()` to predict an `sdf` containing a single vector of features as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:46.908619Z",
     "start_time": "2022-07-18T07:53:46.226695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " features   | [9.3,0.0,0.0,0.97,2.0] \n",
      " prediction | 13.738320482070215     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lm.transform(predict_sdf)\n",
    "predictions.show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the original record and compare the prediction vs true value.\n",
    "\n",
    "For evaluation metrics, you can use the `.summary` method. See https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegressionModel.html?highlight=ml%20summary%20regression#pyspark.ml.regression.LinearRegressionModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:46.923654Z",
     "start_time": "2022-07-18T07:53:46.909422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9939850653121853"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r2 example\n",
    "lm.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:46.928390Z",
     "start_time": "2022-07-18T07:53:46.924481Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LinearRegressionTrainingSummary in module pyspark.ml.regression object:\n",
      "\n",
      "class LinearRegressionTrainingSummary(LinearRegressionSummary)\n",
      " |  LinearRegressionTrainingSummary(java_obj: Optional[ForwardRef('JavaObject')] = None)\n",
      " |  \n",
      " |  Linear regression training results. Currently, the training summary ignores the\n",
      " |  training weights except for the objective trace.\n",
      " |  \n",
      " |  .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegressionTrainingSummary\n",
      " |      LinearRegressionSummary\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  objectiveHistory\n",
      " |      Objective function (scaled loss + regularization) at each\n",
      " |      iteration.\n",
      " |      This value is only available when using the \"l-bfgs\" solver.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      LinearRegression.solver\n",
      " |  \n",
      " |  totalIterations\n",
      " |      Number of training iterations until termination.\n",
      " |      This value is only available when using the \"l-bfgs\" solver.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      LinearRegression.solver\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from LinearRegressionSummary:\n",
      " |  \n",
      " |  coefficientStandardErrors\n",
      " |      Standard error of estimated coefficients and intercept.\n",
      " |      This value is only available when using the \"normal\" solver.\n",
      " |      \n",
      " |      If :py:attr:`LinearRegression.fitIntercept` is set to True,\n",
      " |      then the last element returned corresponds to the intercept.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      LinearRegression.solver\n",
      " |  \n",
      " |  degreesOfFreedom\n",
      " |      Degrees of freedom.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |  \n",
      " |  devianceResiduals\n",
      " |      The weighted residuals, the usual residuals rescaled by the\n",
      " |      square root of the instance weights.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  explainedVariance\n",
      " |      Returns the explained variance regression score.\n",
      " |      explainedVariance = :math:`1 - \\frac{variance(y - \\hat{y})}{variance(y)}`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This ignores instance weights (setting all to 1.0) from\n",
      " |      `LinearRegression.weightCol`. This will change in later Spark\n",
      " |      versions.\n",
      " |      \n",
      " |      For additional information see\n",
      " |      `Explained variation on Wikipedia \\\n",
      " |      <http://en.wikipedia.org/wiki/Explained_variation>`_\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  featuresCol\n",
      " |      Field in \"predictions\" which gives the features of each instance\n",
      " |      as a vector.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  labelCol\n",
      " |      Field in \"predictions\" which gives the true label of each\n",
      " |      instance.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  meanAbsoluteError\n",
      " |      Returns the mean absolute error, which is a risk function\n",
      " |      corresponding to the expected value of the absolute error\n",
      " |      loss or l1-norm loss.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This ignores instance weights (setting all to 1.0) from\n",
      " |      `LinearRegression.weightCol`. This will change in later Spark\n",
      " |      versions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  meanSquaredError\n",
      " |      Returns the mean squared error, which is a risk function\n",
      " |      corresponding to the expected value of the squared error\n",
      " |      loss or quadratic loss.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This ignores instance weights (setting all to 1.0) from\n",
      " |      `LinearRegression.weightCol`. This will change in later Spark\n",
      " |      versions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  numInstances\n",
      " |      Number of instances in DataFrame predictions\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  pValues\n",
      " |      Two-sided p-value of estimated coefficients and intercept.\n",
      " |      This value is only available when using the \"normal\" solver.\n",
      " |      \n",
      " |      If :py:attr:`LinearRegression.fitIntercept` is set to True,\n",
      " |      then the last element returned corresponds to the intercept.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      LinearRegression.solver\n",
      " |  \n",
      " |  predictionCol\n",
      " |      Field in \"predictions\" which gives the predicted value of\n",
      " |      the label at each instance.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  predictions\n",
      " |      Dataframe outputted by the model's `transform` method.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  r2\n",
      " |      Returns R^2, the coefficient of determination.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This ignores instance weights (setting all to 1.0) from\n",
      " |      `LinearRegression.weightCol`. This will change in later Spark\n",
      " |      versions.\n",
      " |      \n",
      " |      See also `Wikipedia coefficient of determination         <http://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  r2adj\n",
      " |      Returns Adjusted R^2, the adjusted coefficient of determination.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This ignores instance weights (setting all to 1.0) from\n",
      " |      `LinearRegression.weightCol`. This will change in later Spark versions.\n",
      " |      \n",
      " |      `Wikipedia coefficient of determination, Adjusted R^2         <https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2>`_\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  residuals\n",
      " |      Residuals (label - predicted value)\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  rootMeanSquaredError\n",
      " |      Returns the root mean squared error, which is defined as the\n",
      " |      square root of the mean squared error.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This ignores instance weights (setting all to 1.0) from\n",
      " |      `LinearRegression.weightCol`. This will change in later Spark\n",
      " |      versions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  tValues\n",
      " |      T-statistic of estimated coefficients and intercept.\n",
      " |      This value is only available when using the \"normal\" solver.\n",
      " |      \n",
      " |      If :py:attr:`LinearRegression.fitIntercept` is set to True,\n",
      " |      then the last element returned corresponds to the intercept.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      LinearRegression.solver\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self) -> None\n",
      " |  \n",
      " |  __init__(self, java_obj: Optional[ForwardRef('JavaObject')] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you want to see the definitive list of all evaluation metrics accessible from lm.summary\n",
    "help(lm.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultation and Questions for Project 1\n",
    "- Due to feedback and the number of questions on Discord, we'll finish the tute here for now and focus on answering questions related to Project 1. \n",
    "- If you would like to ask feedback for your approach and/or report, feel free to ask your tutor to assist if time permits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) General Notes for Revision\n",
    "#### What is the Bias-Variance tradeoff with respect to linear models:\n",
    "- Less parameters = less variance but more bias\n",
    "- More parameters = more variance but less bias\n",
    "- The goal depends on the problem, but generally we want an even variance and bias (intersection).\n",
    "\n",
    "\n",
    "#### Is using regression on X attribute / specific dataset even a good choice...?\n",
    "- The answer is yes, it is a good choice *to try*\n",
    "- BUT also try other methods...\n",
    "\n",
    "\n",
    "#### What are the pros and cons of stepwise regression?\n",
    "- Forward Selection (start from nothing and end until significant)\n",
    "- Backward Elimination (start with everything and end until no more can be removed)\n",
    "- Not always the best results...\n",
    "\n",
    "\n",
    "#### What is best subset regression and the pros and cons of it?\n",
    "- A brute-force like method of fitting *all possible regressions* or *all possible models*\n",
    "- Unlike stepwise, this method fits all possible models based on the variables specified, so you will get the best model possible\n",
    "![a_secret_easter_egg_i_like_apples](https://i.kym-cdn.com/photos/images/newsfeed/001/718/138/147.jpg)\n",
    "\n",
    "\n",
    "\n",
    "#### What is an assumption we make when we fit linear regression models?\n",
    "- Make sure the input matrix is full rank.\n",
    "  - Question: _What happens when input matrix is not full rank?_\n",
    "- Well, the data has to be linearly separable. \n",
    "- Does this also apply to other models too...? (Recall SVM and kernel function which we can use)\n",
    "- Perhaps another model might suit the dataset... (Trees, Neural Networks, Clustering, etc...)\n",
    "\n",
    "\n",
    "#### If you were to use a decision tree, how would you compare between two different fits? \n",
    "- Look at Gini Impurity (probability of an incorrectly classified instance)\n",
    "\n",
    "\n",
    "#### How about baselines or other predictive machine learning models?\n",
    "- Precision, Recall, Classification Accuracy...\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Fitting a GLM with Python\n",
    "**Since MAST30027 is not a pre-requisuite, we will not cover this nor do we expect students to use GLMs. However, those who wish to experiment with GLMs using Python may go through this section.**\n",
    "\n",
    "Let's go through an example:\n",
    "- The `passenger_count` attribute is discrete and non-negative. If we were to predict it, a linear model will not be sufficient. \n",
    "- We know that a Poisson distribution takes in non-negative integer values, so we can use the Poisson family of GLMs to model this. \n",
    "- We will use `total_amount, trip_distance, VendorID` as our regressors.\n",
    "\n",
    "Summary:\n",
    "- GLM's allow us to express relationships in a linear and additive way like normal linear regression.\n",
    "- However, it might be the case that the underlying true relationship is neither linear nor additive. \n",
    "- The transformation is done through a *link function* (in this case, Poisson).\n",
    "\n",
    "Why would we use try and use Poisson? The distribution of `passenger_count` is frequency based greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:56:43.429657Z",
     "start_time": "2022-07-18T07:56:43.206205Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m families\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# convert VendorID to categorical\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVendorID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVendorID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# statsmodels glm\u001b[39;00m\n\u001b[1;32m      7\u001b[0m fit \u001b[38;5;241m=\u001b[39m glm(\n\u001b[1;32m      8\u001b[0m     formula\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassenger_count ~ total_amount + trip_distance + VendorID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     data\u001b[38;5;241m=\u001b[39mdf, \n\u001b[1;32m     10\u001b[0m     family\u001b[38;5;241m=\u001b[39mfamilies\u001b[38;5;241m.\u001b[39mPoisson()\n\u001b[1;32m     11\u001b[0m )\u001b[38;5;241m.\u001b[39mfit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from statsmodels.api import families\n",
    "\n",
    "# convert VendorID to categorical\n",
    "df['VendorID'] = df['VendorID'] == 1\n",
    "\n",
    "# statsmodels glm\n",
    "fit = glm(\n",
    "    formula=\"passenger_count ~ total_amount + trip_distance + VendorID\",\n",
    "    data=df, \n",
    "    family=families.Poisson()\n",
    ").fit()\n",
    "\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Discussion Questions\n",
    "**Solutions will be released in Week 4.**\n",
    "\n",
    "These are some optional class discussion questions or left as extra study materials. Feel free to come back to these after this subject - these are common interview questions for Data Science.\n",
    "\n",
    "1. What is formula of $f_{\\beta}(x)$, $\\ell(\\cdot, \\cdot)$ for linear regression?\n",
    "\n",
    "    1. Can you also define them for LASSO/Ridge regressions?\n",
    "\n",
    "\n",
    "2. Ridge Regression tends to penalize coefficients to a small value, but, not zero. Can you explain this?\n",
    "    - Tip: consider a pair of positive coefficient $\\beta=[1, \\epsilon], \\epsilon < 1$, and think about what happens to $||\\beta||_2^2$ when you subtract $\\delta \\leq \\epsilon$ from each element $\\beta$\n",
    "    \n",
    "\n",
    "3. How would you implement optimizer for the following models: LASSO, Ridge, Elastic Net?\n",
    "\n",
    "4. When using penalised regression models, do you need to standardize the input? Why?\n",
    "\n",
    "5. Is the following pseudo-code correct? Why or why not?\n",
    "\n",
    "```R\n",
    "mu <- mean(X)\n",
    "sigma <- std(X)\n",
    "\n",
    "X_std <- (X - mu) / sigma\n",
    "X_train, X_test, y_train, y_test <- split(X_std, y)\n",
    "\n",
    "model <- fit(y_train ~ X_train)\n",
    "y_pred <- predict(model, X_test)\n",
    "\n",
    "loss <- MSE(y_test, y_pred)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "965cc624ba6b9ab1977c3e5c7cf73bafd73873218edb1037405eebf98d8eb6cb"
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit ('3.11.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1c6f7b18ea35922dad1f927d5d0123541ee5478d7a9729c6a2c6ed680be427a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
